#!/usr/bin/env python3
"""Simple test of LM Studio connection and basic AI interaction."""

import asyncio
import aiohttp
import json


async def test_lm_studio_simple():
    """Simple test of LM Studio connectivity."""
    base_url = "http://localhost:1234/v1"
    
    async with aiohttp.ClientSession() as session:
        print("ğŸ¤– Testing LM Studio Connection")
        print("=" * 40)
        
        # Test 1: Check models
        print("\nğŸ“‹ Step 1: Check Available Models")
        try:
            async with session.get(f"{base_url}/models") as response:
                if response.status == 200:
                    models = await response.json()
                    model_list = models.get("data", [])
                    if model_list:
                        model_name = model_list[0]["id"]
                        print(f"âœ… Model loaded: {model_name}")
                    else:
                        print("âŒ No models loaded")
                        return False
                else:
                    print(f"âŒ Failed to get models (status: {response.status})")
                    return False
        except Exception as e:
            print(f"âŒ Connection failed: {e}")
            return False
        
        # Test 2: Simple chat completion
        print(f"\nğŸ§  Step 2: Test AI Chat Completion")
        simple_request = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": "Say 'Hello from Qwen2.5 Coder!' and nothing else."}
            ],
            "temperature": 0.1,
            "max_tokens": 50
        }
        
        try:
            async with session.post(f"{base_url}/chat/completions", json=simple_request) as response:
                if response.status == 200:
                    result = await response.json()
                    ai_response = result["choices"][0]["message"]["content"]
                    print(f"âœ… AI Response: {ai_response}")
                else:
                    print(f"âŒ Chat completion failed (status: {response.status})")
                    return False
        except Exception as e:
            print(f"âŒ Chat completion error: {e}")
            return False
        
        # Test 3: Python code generation
        print(f"\nğŸ’» Step 3: Test Python Code Generation")
        code_request = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": "Write a simple Python function that adds two numbers. Only code, no explanations."}
            ],
            "temperature": 0.1,
            "max_tokens": 100
        }
        
        try:
            async with session.post(f"{base_url}/chat/completions", json=code_request) as response:
                if response.status == 200:
                    result = await response.json()
                    code_response = result["choices"][0]["message"]["content"]
                    print(f"âœ… Generated code:")
                    print(f"   {code_response[:100]}...")
                else:
                    print(f"âŒ Code generation failed (status: {response.status})")
                    return False
        except Exception as e:
            print(f"âŒ Code generation error: {e}")
            return False
        
        print("\n" + "=" * 40)
        print("ğŸ‰ LM Studio Integration Test PASSED!")
        print(f"\nSummary:")
        print(f"  âœ… LM Studio Server: Connected")
        print(f"  âœ… Model: {model_name}")
        print(f"  âœ… Chat Completion: Working")
        print(f"  âœ… Code Generation: Working")
        print(f"\nğŸš€ Ready for Blender integration!")
        
        return True


if __name__ == "__main__":
    success = asyncio.run(test_lm_studio_simple())
    if not success:
        exit(1)